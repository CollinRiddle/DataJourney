{
  "config_version": "1.0",
  "project_info": {
    "name": "DataJourney",
    "description": "Educational project showcasing data pipeline workflows",
    "author": "Collin Riddle",
    "created_date": ""
  },
  "database": {
    "type": "postgresql",
    "host": "${DB_HOST}",
    "port": "${DB_PORT}",
    "database_name": "${DB_NAME}",
    "schema": "public",
    "connection_note": "Credentials loaded from environment variables"
  },
  "pipelines": [
    {
      "pipeline_id": "covid_analysis",
      "pipeline_name": "COVID-19 Data Analysis",
      "description": "Fetches COVID-19 data from API and processes it for analysis",
      "source_type": "api",
      "status": "active",
      "last_run": "2025-10-06T14:30:00Z",
      "stages": [
        {
          "stage_id": "fetch_covid_api",
          "stage_number": 1,
          "stage_name": "Fetch COVID-19 API Data",
          "stage_type": "data_ingestion",
          "description": "Retrieve COVID-19 statistics from public API",
          "source": {
            "type": "api",
            "url": "https://api.covidtracking.com/v1/us/daily.json",
            "method": "GET",
            "headers": {
              "Content-Type": "application/json"
            },
            "authentication": "none",
            "response_format": "json"
          },
          "output": {
            "format": "dataframe",
            "row_count": 500,
            "columns": ["date", "positive", "negative", "death", "hospitalizedCurrently"],
            "sample_data": [
              {"date": "20210315", "positive": 29500000, "negative": 300000000, "death": 535000, "hospitalizedCurrently": 40000},
              {"date": "20210314", "positive": 29400000, "negative": 299000000, "death": 534000, "hospitalizedCurrently": 41000}
            ]
          },
          "code_snippet": "import requests\nimport pandas as pd\n\nresponse = requests.get('https://api.covidtracking.com/v1/us/daily.json')\ndata = response.json()\ndf = pd.DataFrame(data)",
          "execution_time_ms": 450,
          "notes": "API provides historical daily data for US states"
        },
        {
          "stage_id": "clean_covid_data",
          "stage_number": 2,
          "stage_name": "Clean and Format Data",
          "stage_type": "data_cleaning",
          "description": "Remove null values, convert date format, filter invalid records",
          "transformations": [
            {
              "operation": "remove_nulls",
              "columns": ["positive", "death"],
              "description": "Remove rows where critical columns are null"
            },
            {
              "operation": "convert_date",
              "column": "date",
              "from_format": "YYYYMMDD",
              "to_format": "YYYY-MM-DD",
              "description": "Convert date from integer format to standard date string"
            },
            {
              "operation": "filter_rows",
              "condition": "positive > 0",
              "description": "Keep only records with positive case counts"
            }
          ],
          "input": {
            "row_count": 500,
            "columns": ["date", "positive", "negative", "death", "hospitalizedCurrently"]
          },
          "output": {
            "format": "dataframe",
            "row_count": 487,
            "columns": ["date", "positive", "negative", "death", "hospitalizedCurrently"],
            "sample_data": [
              {"date": "2021-03-15", "positive": 29500000, "negative": 300000000, "death": 535000, "hospitalizedCurrently": 40000},
              {"date": "2021-03-14", "positive": 29400000, "negative": 299000000, "death": 534000, "hospitalizedCurrently": 41000}
            ]
          },
          "code_snippet": "# Remove null values\ndf = df.dropna(subset=['positive', 'death'])\n\n# Convert date format\ndf['date'] = pd.to_datetime(df['date'], format='%Y%m%d')\n\n# Filter invalid records\ndf = df[df['positive'] > 0]",
          "execution_time_ms": 120,
          "rows_removed": 13,
          "notes": "Data quality improved significantly after cleaning"
        },
        {
          "stage_id": "aggregate_covid_data",
          "stage_number": 3,
          "stage_name": "Aggregate Weekly Statistics",
          "stage_type": "data_transformation",
          "description": "Calculate weekly averages and trends",
          "transformations": [
            {
              "operation": "add_week_column",
              "description": "Extract week number from date"
            },
            {
              "operation": "group_by",
              "columns": ["week"],
              "aggregations": {
                "positive": "sum",
                "death": "sum",
                "hospitalizedCurrently": "mean"
              },
              "description": "Calculate weekly totals and averages"
            },
            {
              "operation": "calculate_derived",
              "new_column": "mortality_rate",
              "formula": "death / positive",
              "description": "Calculate mortality rate percentage"
            }
          ],
          "input": {
            "row_count": 487,
            "columns": ["date", "positive", "negative", "death", "hospitalizedCurrently"]
          },
          "output": {
            "format": "dataframe",
            "row_count": 70,
            "columns": ["week", "positive_total", "death_total", "avg_hospitalized", "mortality_rate"],
            "sample_data": [
              {"week": 11, "positive_total": 1500000, "death_total": 25000, "avg_hospitalized": 39500, "mortality_rate": 0.0167},
              {"week": 10, "positive_total": 1450000, "death_total": 24500, "avg_hospitalized": 41000, "mortality_rate": 0.0169}
            ]
          },
          "code_snippet": "# Add week column\ndf['week'] = df['date'].dt.isocalendar().week\n\n# Group by week and aggregate\nweekly_df = df.groupby('week').agg({\n    'positive': 'sum',\n    'death': 'sum',\n    'hospitalizedCurrently': 'mean'\n}).reset_index()\n\n# Calculate mortality rate\nweekly_df['mortality_rate'] = weekly_df['death'] / weekly_df['positive']",
          "execution_time_ms": 85,
          "notes": "Weekly aggregation provides better trend visibility"
        },
        {
          "stage_id": "load_to_database",
          "stage_number": 4,
          "stage_name": "Load to PostgreSQL",
          "stage_type": "data_loading",
          "description": "Store processed data in PostgreSQL database",
          "destination": {
            "type": "postgresql",
            "table_name": "covid_weekly_stats",
            "write_mode": "replace",
            "create_indexes": ["week"]
          },
          "input": {
            "row_count": 70,
            "columns": ["week", "positive_total", "death_total", "avg_hospitalized", "mortality_rate"]
          },
          "output": {
            "rows_inserted": 70,
            "table_name": "covid_weekly_stats",
            "status": "success"
          },
          "code_snippet": "from sqlalchemy import create_engine\n\n# Create database engine from environment variable\nengine = create_engine(os.environ['DATABASE_URL'])\n\n# Write to PostgreSQL\nweekly_df.to_sql(\n    'covid_weekly_stats',\n    engine,\n    if_exists='replace',\n    index=False\n)",
          "execution_time_ms": 320,
          "notes": "Data successfully loaded to production database"
        }
      ],
      "total_execution_time_ms": 975,
      "final_output": {
        "database_table": "covid_weekly_stats",
        "record_count": 70
      }
    },
    {
      "pipeline_id": "kaggle_housing",
      "pipeline_name": "Housing Price Analysis",
      "description": "Process Kaggle housing dataset for price prediction analysis",
      "source_type": "file",
      "status": "active",
      "last_run": "2025-10-05T09:15:00Z",
      "stages": [
        {
          "stage_id": "load_kaggle_csv",
          "stage_number": 1,
          "stage_name": "Load Kaggle CSV",
          "stage_type": "data_ingestion",
          "description": "Read housing data from Kaggle CSV file",
          "source": {
            "type": "file",
            "file_path": "./data/housing_prices.csv",
            "file_format": "csv",
            "encoding": "utf-8",
            "delimiter": ","
          },
          "output": {
            "format": "dataframe",
            "row_count": 1460,
            "columns": ["Id", "MSSubClass", "LotArea", "OverallQual", "YearBuilt", "SalePrice"],
            "sample_data": [
              {"Id": 1, "MSSubClass": 60, "LotArea": 8450, "OverallQual": 7, "YearBuilt": 2003, "SalePrice": 208500},
              {"Id": 2, "MSSubClass": 20, "LotArea": 9600, "OverallQual": 6, "YearBuilt": 1976, "SalePrice": 181500}
            ]
          },
          "code_snippet": "import pandas as pd\n\n# Read CSV file\ndf = pd.read_csv('./data/housing_prices.csv')\n\nprint(f'Loaded {len(df)} rows')",
          "execution_time_ms": 125,
          "notes": "Dataset contains 1460 houses with 81 features"
        },
        {
          "stage_id": "clean_housing_data",
          "stage_number": 2,
          "stage_name": "Clean Housing Data",
          "stage_type": "data_cleaning",
          "description": "Handle missing values and outliers",
          "transformations": [
            {
              "operation": "fill_missing",
              "columns": ["LotFrontage"],
              "strategy": "median",
              "description": "Fill missing lot frontage with median value"
            },
            {
              "operation": "drop_columns",
              "columns": ["Alley", "PoolQC", "Fence", "MiscFeature"],
              "reason": "Too many missing values (>80%)",
              "description": "Remove columns with excessive missing data"
            },
            {
              "operation": "remove_outliers",
              "column": "SalePrice",
              "method": "iqr",
              "threshold": 3,
              "description": "Remove extreme price outliers using IQR method"
            }
          ],
          "input": {
            "row_count": 1460,
            "columns": 81
          },
          "output": {
            "format": "dataframe",
            "row_count": 1438,
            "columns": 77,
            "sample_data": [
              {"Id": 1, "MSSubClass": 60, "LotArea": 8450, "OverallQual": 7, "YearBuilt": 2003, "SalePrice": 208500},
              {"Id": 2, "MSSubClass": 20, "LotArea": 9600, "OverallQual": 6, "YearBuilt": 1976, "SalePrice": 181500}
            ]
          },
          "code_snippet": "# Fill missing values\ndf['LotFrontage'].fillna(df['LotFrontage'].median(), inplace=True)\n\n# Drop columns with too many missing values\ndf = df.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n\n# Remove outliers\nQ1 = df['SalePrice'].quantile(0.25)\nQ3 = df['SalePrice'].quantile(0.75)\nIQR = Q3 - Q1\ndf = df[~((df['SalePrice'] < (Q1 - 3 * IQR)) | (df['SalePrice'] > (Q3 + 3 * IQR)))]",
          "execution_time_ms": 95,
          "rows_removed": 22,
          "columns_removed": 4,
          "notes": "Cleaned dataset ready for analysis"
        },
        {
          "stage_id": "feature_engineering",
          "stage_number": 3,
          "stage_name": "Feature Engineering",
          "stage_type": "data_transformation",
          "description": "Create new features for analysis",
          "transformations": [
            {
              "operation": "create_feature",
              "new_column": "HouseAge",
              "formula": "2025 - YearBuilt",
              "description": "Calculate house age from year built"
            },
            {
              "operation": "create_feature",
              "new_column": "TotalSF",
              "formula": "TotalBsmtSF + 1stFlrSF + 2ndFlrSF",
              "description": "Calculate total square footage"
            },
            {
              "operation": "encode_categorical",
              "columns": ["Neighborhood", "HouseStyle"],
              "method": "one_hot",
              "description": "Convert categorical variables to numeric"
            }
          ],
          "input": {
            "row_count": 1438,
            "columns": 77
          },
          "output": {
            "format": "dataframe",
            "row_count": 1438,
            "columns": 95,
            "sample_data": [
              {"Id": 1, "HouseAge": 22, "TotalSF": 2566, "SalePrice": 208500},
              {"Id": 2, "HouseAge": 49, "TotalSF": 2200, "SalePrice": 181500}
            ]
          },
          "code_snippet": "# Calculate house age\ndf['HouseAge'] = 2025 - df['YearBuilt']\n\n# Calculate total square footage\ndf['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\n# One-hot encode categorical variables\ndf = pd.get_dummies(df, columns=['Neighborhood', 'HouseStyle'])",
          "execution_time_ms": 180,
          "notes": "Added 18 new columns through feature engineering"
        },
        {
          "stage_id": "load_housing_db",
          "stage_number": 4,
          "stage_name": "Load to Database",
          "stage_type": "data_loading",
          "description": "Store processed housing data in PostgreSQL",
          "destination": {
            "type": "postgresql",
            "table_name": "housing_features",
            "write_mode": "replace",
            "create_indexes": ["Id", "HouseAge"]
          },
          "input": {
            "row_count": 1438,
            "columns": 95
          },
          "output": {
            "rows_inserted": 1438,
            "table_name": "housing_features",
            "status": "success"
          },
          "code_snippet": "from sqlalchemy import create_engine\n\nengine = create_engine(os.environ['DATABASE_URL'])\n\ndf.to_sql(\n    'housing_features',\n    engine,\n    if_exists='replace',\n    index=False\n)",
          "execution_time_ms": 450,
          "notes": "Housing features ready for ML model training"
        }
      ],
      "total_execution_time_ms": 850,
      "final_output": {
        "database_table": "housing_features",
        "record_count": 1438
      }
    },
    {
      "pipeline_id": "jira_issues",
      "pipeline_name": "Jira Issues Analysis",
      "description": "Extract and analyze Jira issue data via API",
      "source_type": "api",
      "status": "development",
      "last_run": null,
      "stages": [
        {
          "stage_id": "fetch_jira_api",
          "stage_number": 1,
          "stage_name": "Fetch Jira Issues",
          "stage_type": "data_ingestion",
          "description": "Query Jira API for project issues",
          "source": {
            "type": "api",
            "url": "${JIRA_URL}/rest/api/3/search",
            "method": "POST",
            "headers": {
              "Content-Type": "application/json",
              "Authorization": "${JIRA_AUTH_HEADER}"
            },
            "authentication": "basic",
            "auth_note": "Requires email and API token. Set JIRA_AUTH_HEADER with base64 encoded credentials",
            "request_body": {
              "jql": "${JIRA_QUERY}",
              "maxResults": 100,
              "fields": ["summary", "status", "assignee", "created", "priority"]
            },
            "response_format": "json"
          },
          "output": {
            "format": "dataframe",
            "row_count": 87,
            "columns": ["issue_key", "summary", "status", "assignee", "created", "priority"],
            "sample_data": [
              {"issue_key": "PROJ-123", "summary": "Fix login bug", "status": "In Progress", "assignee": "john.doe", "created": "2025-09-15", "priority": "High"},
              {"issue_key": "PROJ-124", "summary": "Update documentation", "status": "To Do", "assignee": "jane.smith", "created": "2025-09-20", "priority": "Medium"}
            ]
          },
          "code_snippet": "import requests\nimport os\nimport pandas as pd\n\n# Get credentials from environment\njira_url = os.environ['JIRA_URL']\nauth_header = os.environ['JIRA_AUTH_HEADER']\njql_query = os.environ.get('JIRA_QUERY', 'project = PROJ')\n\n# Query Jira API\nresponse = requests.post(\n    f'{jira_url}/rest/api/3/search',\n    headers={'Authorization': auth_header},\n    json={'jql': jql_query, 'maxResults': 100}\n)\n\ndata = response.json()['issues']\ndf = pd.DataFrame(data)",
          "execution_time_ms": 850,
          "notes": "JQL query returns open issues from specified project"
        },
        {
          "stage_id": "parse_jira_response",
          "stage_number": 2,
          "stage_name": "Parse Jira Response",
          "stage_type": "data_transformation",
          "description": "Extract and flatten nested JSON structure",
          "transformations": [
            {
              "operation": "flatten_json",
              "description": "Extract fields from nested JSON structure"
            },
            {
              "operation": "parse_datetime",
              "column": "created",
              "description": "Convert ISO datetime strings to datetime objects"
            },
            {
              "operation": "extract_field",
              "path": "fields.assignee.displayName",
              "new_column": "assignee_name",
              "description": "Extract assignee display name from nested object"
            }
          ],
          "input": {
            "row_count": 87,
            "columns": ["key", "fields"]
          },
          "output": {
            "format": "dataframe",
            "row_count": 87,
            "columns": ["issue_key", "summary", "status", "assignee_name", "created", "priority"],
            "sample_data": [
              {"issue_key": "PROJ-123", "summary": "Fix login bug", "status": "In Progress", "assignee_name": "John Doe", "created": "2025-09-15T10:30:00Z", "priority": "High"}
            ]
          },
          "code_snippet": "# Flatten nested structure\ndf['issue_key'] = df['key']\ndf['summary'] = df['fields'].apply(lambda x: x['summary'])\ndf['status'] = df['fields'].apply(lambda x: x['status']['name'])\ndf['assignee_name'] = df['fields'].apply(lambda x: x['assignee']['displayName'] if x.get('assignee') else None)\n\n# Parse datetime\ndf['created'] = pd.to_datetime(df['fields'].apply(lambda x: x['created']))",
          "execution_time_ms": 65,
          "notes": "Jira response structure successfully normalized"
        },
        {
          "stage_id": "load_jira_db",
          "stage_number": 3,
          "stage_name": "Load to Database",
          "stage_type": "data_loading",
          "description": "Store Jira issues in PostgreSQL",
          "destination": {
            "type": "postgresql",
            "table_name": "jira_issues",
            "write_mode": "append",
            "create_indexes": ["issue_key", "created"]
          },
          "input": {
            "row_count": 87,
            "columns": ["issue_key", "summary", "status", "assignee_name", "created", "priority"]
          },
          "output": {
            "rows_inserted": 87,
            "table_name": "jira_issues",
            "status": "success"
          },
          "code_snippet": "from sqlalchemy import create_engine\nimport os\n\nengine = create_engine(os.environ['DATABASE_URL'])\n\ndf.to_sql(\n    'jira_issues',\n    engine,\n    if_exists='append',\n    index=False\n)",
          "execution_time_ms": 280,
          "notes": "Jira issues available for reporting and analysis"
        }
      ],
      "total_execution_time_ms": 1195,
      "final_output": {
        "database_table": "jira_issues",
        "record_count": 87
      }
    }
  ],
  "visualization_settings": {
    "dag_layout": "horizontal",
    "node_styling": {
      "data_ingestion": {"color": "#4CAF50", "icon": "download"},
      "data_cleaning": {"color": "#2196F3", "icon": "clean"},
      "data_transformation": {"color": "#FF9800", "icon": "transform"},
      "data_loading": {"color": "#9C27B0", "icon": "database"}
    },
    "edge_styling": {
      "stroke": "#666",
      "stroke_width": 2,
      "arrow_size": 10
    },
    "interaction": {
      "enable_click": true,
      "enable_hover": true,
      "show_tooltips": true
    }
  }
}