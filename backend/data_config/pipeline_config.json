{
  "config_version": "1.0",
  "project_info": {
    "name": "Data Pipeline Visualization Project",
    "description": "Educational project showcasing data pipeline workflows",
    "author": "Your Name",
    "created_date": "2025-10-07"
  },
  "pipelines": [
    {
      "pipeline_id": "thailand_hotels",
      "pipeline_name": "Thailand Hotel Listings",
      "description": "Extract hotel data from Kaggle, transform, and load to PostgreSQL",
      "source_type": "file",
      "status": "active",
      "last_run": null,
      "detailed_explanation": {
        "overview": "This beginner-friendly pipeline downloads real hotel data from Kaggle, cleans it up, and stores it in a database. It's a simple, linear workflow that demonstrates the core ETL (Extract, Transform, Load) pattern.",
        "what_you_learn": [
          "How to download datasets from Kaggle using kagglehub",
          "Basic data cleaning techniques (renaming columns, parsing values)",
          "Loading data into PostgreSQL databases"
        ],
        "stage_details": [
          {
            "stage_number": 1,
            "what_happens": "The pipeline connects to Kaggle and downloads a CSV file containing information about resorts in Thailand. This file includes resort names, locations, prices, ratings, and room details.",
            "why_it_matters": "External data sources like Kaggle are common in real-world projects. Learning to programmatically download datasets saves time and enables automation.",
            "technical_note": "Uses the kagglehub library to authenticate and download the dataset to your local cache."
          },
          {
            "stage_number": 2,
            "what_happens": "Raw column names are messy (like 'Name of Resort' or 'Unnamed: 0'). We rename them to clean, database-friendly names (like 'resort_name' and 'id'). We also extract numeric values from text - for example, converting 'US$150' to the number 150.",
            "why_it_matters": "Clean data is easier to work with. Standardized column names prevent errors, and extracting numbers from text enables mathematical operations like sorting by price.",
            "technical_note": "Uses pandas string operations and regular expressions to parse formatted text into structured data."
          },
          {
            "stage_number": 3,
            "what_happens": "The cleaned data is sent to a PostgreSQL database table called 'hotel_listings'. Each row becomes a record in the database, ready to be queried by applications.",
            "why_it_matters": "Databases provide permanent storage and fast querying. Once loaded, this data can power websites, analytics dashboards, or machine learning models.",
            "technical_note": "Uses SQLAlchemy to connect to PostgreSQL and pandas.to_sql() to insert records efficiently."
          }
        ]
      },
      "stages": [
        {
          "stage_id": "extract_kaggle_data",
          "stage_number": 1,
          "stage_name": "Extract from Kaggle",
          "stage_type": "data_ingestion",
          "description": "Download and load Thailand hotel dataset from KaggleHub",
          "source": {
            "type": "kagglehub",
            "dataset_id": "aakashshinde1507/resorts-in-thailand",
            "file_format": "csv"
          },
          "output": {
            "format": "dataframe",
            "columns": [
              "Unnamed: 0",
              "Name of Resort",
              "Place",
              "room",
              "bed",
              "Condition",
              "price",
              "Travel Sustainable Level",
              "Rating",
              "Total Reviews"
            ]
          },
          "code_snippet": "import kagglehub\nimport pandas as pd\nimport os\n\n# Download dataset\npath = kagglehub.dataset_download('aakashshinde1507/resorts-in-thailand')\n\n# Find and read CSV\ncsv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\ncsv_path = os.path.join(path, csv_files[0])\ndf = pd.read_csv(csv_path)",
          "execution_time_ms": null,
          "notes": "Dataset downloaded from Kaggle using kagglehub library"
        },
        {
          "stage_id": "transform_hotel_data",
          "stage_number": 2,
          "stage_name": "Clean and Transform",
          "stage_type": "data_transformation",
          "description": "Clean column names, parse price values, and extract review counts",
          "transformations": [
            {
              "operation": "rename_columns",
              "description": "Rename columns to snake_case format for database compatibility",
              "mapping": {
                "Unnamed: 0": "id",
                "Name of Resort": "resort_name",
                "Place": "location",
                "room": "room_type",
                "bed": "bed_details",
                "Condition": "condition",
                "price": "price",
                "Travel Sustainable Level": "sustainability_level",
                "Rating": "rating",
                "Total Reviews": "total_reviews"
              }
            },
            {
              "operation": "parse_price",
              "column": "price",
              "description": "Extract numeric price from 'US$32' format, handle null values",
              "target_column": "price_usd"
            },
            {
              "operation": "extract_review_count",
              "column": "total_reviews",
              "description": "Extract numeric review count from '100 reviews' format",
              "target_column": "review_count"
            }
          ],
          "input": {
            "columns": [
              "Unnamed: 0",
              "Name of Resort",
              "Place",
              "room",
              "bed",
              "Condition",
              "price",
              "Travel Sustainable Level",
              "Rating",
              "Total Reviews"
            ]
          },
          "output": {
            "format": "dataframe",
            "columns": [
              "id",
              "resort_name",
              "location",
              "room_type",
              "bed_details",
              "condition",
              "price_usd",
              "sustainability_level",
              "rating",
              "review_count"
            ]
          },
          "code_snippet": "# Rename columns\ndf = df.rename(columns={\n    'Unnamed: 0': 'id',\n    'Name of Resort': 'resort_name',\n    'Place': 'location',\n    'room': 'room_type',\n    'bed': 'bed_details',\n    'Condition': 'condition',\n    'price': 'price',\n    'Travel Sustainable Level': 'sustainability_level',\n    'Rating': 'rating',\n    'Total Reviews': 'total_reviews'\n})\n\n# Parse price: 'US$32' -> 32.0\ndf['price_usd'] = df['price'].str.replace('US$', '').str.replace(',', '').astype(float, errors='ignore')\n\n# Extract review count: '100 reviews' -> 100\ndf['review_count'] = df['total_reviews'].str.extract(r'(\\d+)')[0].astype(float)",
          "execution_time_ms": null,
          "notes": "Data cleaned and ready for database insertion"
        },
        {
          "stage_id": "load_to_postgres",
          "stage_number": 3,
          "stage_name": "Load to PostgreSQL",
          "stage_type": "data_loading",
          "description": "Truncate and reload hotel_listings table in PostgreSQL",
          "destination": {
            "table_name": "hotel_listings",
            "write_mode": "replace",
            "create_indexes": true,
            "index_columns": [
              "id",
              "resort_name",
              "location",
              "rating",
              "price_usd"
            ]
          },
          "input": {
            "columns": [
              "id",
              "resort_name",
              "location",
              "room_type",
              "bed_details",
              "condition",
              "price_usd",
              "sustainability_level",
              "rating",
              "review_count"
            ]
          },
          "output": {
            "table_name": "hotel_listings",
            "status": "success"
          },
          "code_snippet": "from sqlalchemy import create_engine, text\nimport os\n\n# Create engine from environment variable\nengine = create_engine(os.environ['AIVEN_PG_URI'])\n\n# Drop and recreate table\nwith engine.connect() as conn:\n    conn.execute(text('DROP TABLE IF EXISTS hotel_listings CASCADE'))\n    conn.commit()\n\n# Load data\ndf.to_sql('hotel_listings', engine, if_exists='replace', index=False)\n\n# Create indexes\nwith engine.connect() as conn:\n    for col in ['id', 'resort_name', 'location', 'rating', 'price_usd']:\n        conn.execute(text(f'CREATE INDEX IF NOT EXISTS idx_{col} ON hotel_listings({col})'))\n    conn.commit()",
          "execution_time_ms": null,
          "notes": "Data successfully loaded to PostgreSQL with indexes"
        }
      ],
      "total_execution_time_ms": null,
      "final_output": {
        "database_table": "hotel_listings",
        "record_count": null
      }
    },
    {
      "pipeline_id": "pokemon_data",
      "pipeline_name": "Pokemon Data Analysis",
      "description": "Extract Pokemon data from PokeAPI, branch processing by legendary status, and load to PostgreSQL",
      "source_type": "api",
      "status": "active",
      "last_run": null,
      "detailed_explanation": {
        "overview": "This intermediate pipeline fetches Pokemon data from a REST API and uses branching logic to process legendary and regular Pokemon differently. It demonstrates conditional workflows where data takes different paths based on its characteristics.",
        "what_you_learn": [
          "Making API calls and handling JSON responses",
          "Implementing branching logic in data pipelines",
          "Calculating derived metrics and enriching data"
        ],
        "stage_details": [
          {
            "stage_number": 1,
            "what_happens": "The pipeline makes 151 API calls to PokeAPI, fetching data for each Generation 1 Pokemon. It retrieves stats (HP, attack, defense), types, and whether each Pokemon is legendary.",
            "why_it_matters": "APIs are the backbone of modern data integration. Learning to handle rate limits, parse JSON, and aggregate API responses is essential for real-world pipelines.",
            "technical_note": "Uses the requests library with error handling and combines data from both pokemon and species endpoints."
          },
          {
            "stage_number": 2,
            "what_happens": "Data splits into two paths: legendary Pokemon get rarity scoring and special roles assigned, while regular Pokemon receive standard categorization. Each path has unique transformations suited to that Pokemon type.",
            "why_it_matters": "Not all data should be processed the same way. Branching logic lets you apply different business rules to different data segments, improving accuracy and relevance.",
            "technical_note": "Uses pandas boolean indexing to filter DataFrames and apply conditional transformations."
          },
          {
            "stage_number": 3,
            "what_happens": "Both data paths merge back together, ensuring all Pokemon end up in the same database table with consistent columns. The final dataset includes both standard and enhanced attributes.",
            "why_it_matters": "Merging branches correctly prevents data loss and maintains schema consistency. This pattern is common in quality control pipelines where items pass or fail checks.",
            "technical_note": "Uses pandas concat() to combine DataFrames while preserving column alignment and handling missing values."
          },
          {
            "stage_number": 4,
            "what_happens": "The merged dataset receives final enrichments: a total power score calculated from all stats, combat role assignment (sweeper, tank, or balanced), and rarity classification based on base experience.",
            "why_it_matters": "Derived metrics add analytical value without requiring additional API calls. These calculated fields enable sorting, filtering, and categorization for downstream applications.",
            "technical_note": "Uses pandas apply() with custom functions to calculate aggregate scores and assign categorical labels."
          },
          {
            "stage_number": 5,
            "what_happens": "The complete Pokemon dataset with stats, types, branching attributes, and derived metrics loads into PostgreSQL as a searchable database table.",
            "why_it_matters": "Database storage enables persistent queries, powers applications like Pokedex tools, and supports analytics dashboards showing stat distributions.",
            "technical_note": "Uses pandas to_sql() with if_exists='replace' to refresh the entire dataset, ensuring data consistency."
          }
        ]
      },
      "stages": [
        {
          "stage_id": "extract_pokeapi",
          "stage_number": 1,
          "stage_name": "Extract from PokeAPI",
          "stage_type": "data_ingestion",
          "description": "Fetch first 151 Pokemon (Gen 1) from PokeAPI including stats, types, and legendary status",
          "source": {
            "type": "api",
            "base_url": "https://pokeapi.co/api/v2",
            "limit": 151
          },
          "output": {
            "format": "dataframe",
            "columns": [
              "pokemon_id",
              "name",
              "height",
              "weight",
              "base_experience",
              "hp",
              "attack",
              "defense",
              "special_attack",
              "special_defense",
              "speed",
              "type_primary",
              "type_secondary",
              "is_legendary",
              "is_mythical",
              "generation"
            ]
          },
          "code_snippet": "import requests\nimport pandas as pd\n\npokemon_list = []\nfor i in range(1, 152):\n    # Fetch pokemon data\n    response = requests.get(f'https://pokeapi.co/api/v2/pokemon/{i}')\n    pokemon_data = response.json()\n    \n    # Fetch species data for legendary status\n    species_url = pokemon_data['species']['url']\n    species_data = requests.get(species_url).json()\n    \n    pokemon = {\n        'pokemon_id': pokemon_data['id'],\n        'name': pokemon_data['name'],\n        'is_legendary': species_data['is_legendary'],\n        # ... more fields\n    }\n    pokemon_list.append(pokemon)\n\ndf = pd.DataFrame(pokemon_list)",
          "execution_time_ms": null,
          "notes": "API calls made to PokeAPI for comprehensive Pokemon data"
        },
        {
          "stage_id": "transform_pokemon",
          "stage_number": 2,
          "stage_name": "Transform & Enrich",
          "stage_type": "data_transformation",
          "description": "Calculate total stats, determine rarity tiers, and format data for analysis",
          "transformations": [
            {
              "operation": "calculate_totals",
              "description": "Sum all base stats to create total_stats field"
            },
            {
              "operation": "assign_rarity",
              "description": "Classify Pokemon as mythical, legendary, rare, uncommon, or common based on stats and status"
            },
            {
              "operation": "format_names",
              "description": "Capitalize Pokemon names for display"
            }
          ],
          "output": {
            "format": "dataframe",
            "additional_columns": [
              "total_stats",
              "rarity",
              "processed_at"
            ]
          },
          "code_snippet": "# Calculate total stats\ndf['total_stats'] = df['hp'] + df['attack'] + df['defense'] + \\\n                     df['special_attack'] + df['special_defense'] + df['speed']\n\n# Determine rarity\ndf['rarity'] = df.apply(lambda row: \n    'mythical' if row['is_mythical']\n    else 'legendary' if row['is_legendary']\n    else 'rare' if row['total_stats'] >= 500\n    else 'uncommon' if row['total_stats'] >= 400\n    else 'common',\n    axis=1\n)\n\n# Format names\ndf['name'] = df['name'].str.title()",
          "execution_time_ms": null,
          "notes": "Data enriched with calculated fields for better analysis"
        },
        {
          "stage_id": "branch_legendary",
          "stage_number": 3,
          "stage_name": "Branch Decision",
          "stage_type": "data_branching",
          "description": "Split dataset into legendary/mythical and non-legendary Pokemon for specialized processing",
          "branch_condition": "is_legendary OR is_mythical",
          "branches": [
            {
              "branch_id": "legendary_path",
              "condition": "legendary == True OR mythical == True",
              "next_stage": "process_legendary"
            },
            {
              "branch_id": "non_legendary_path",
              "condition": "legendary == False AND mythical == False",
              "next_stage": "process_non_legendary"
            }
          ],
          "code_snippet": "# Create boolean mask for legendary/mythical\nlegendary_mask = (df['is_legendary'] == True) | (df['is_mythical'] == True)\n\n# Split into two dataframes\nlegendary_df = df[legendary_mask].copy()\nnon_legendary_df = df[~legendary_mask].copy()\n\nprint(f'Legendary: {len(legendary_df)}, Non-legendary: {len(non_legendary_df)}')",
          "execution_time_ms": null,
          "notes": "Data split into two processing paths based on legendary status"
        },
        {
          "stage_id": "process_legendary",
          "stage_number": 4,
          "stage_name": "Process Legendary",
          "stage_type": "data_transformation",
          "description": "Apply specialized transformations for legendary Pokemon including tier classification and power scoring",
          "branch_path": "legendary",
          "transformations": [
            {
              "operation": "classify_legendary_tier",
              "description": "Categorize as mythical, box legendary, or sub-legendary"
            },
            {
              "operation": "calculate_power_score",
              "description": "Enhanced power calculation with 1.5x multiplier for legendary status"
            }
          ],
          "code_snippet": "# Classify legendary tier\nlegendary_df['legendary_tier'] = legendary_df.apply(lambda row:\n    'mythical' if row['is_mythical']\n    else 'box_legendary' if row['total_stats'] >= 680\n    else 'sub_legendary',\n    axis=1\n)\n\n# Calculate enhanced power score\nlegendary_df['power_score'] = (legendary_df['total_stats'] * 1.5 + \n                                legendary_df['base_experience'])",
          "execution_time_ms": null,
          "notes": "Legendary Pokemon receive enhanced analysis and classification"
        },
        {
          "stage_id": "process_non_legendary",
          "stage_number": 4,
          "stage_name": "Process Non-Legendary",
          "stage_type": "data_transformation",
          "description": "Apply standard transformations for non-legendary Pokemon including combat role classification",
          "branch_path": "non_legendary",
          "transformations": [
            {
              "operation": "calculate_power_score",
              "description": "Standard power calculation for non-legendary Pokemon"
            },
            {
              "operation": "assign_combat_role",
              "description": "Classify as sweeper, tank, or balanced based on stat distribution"
            }
          ],
          "code_snippet": "# Calculate standard power score\nnon_legendary_df['power_score'] = (non_legendary_df['total_stats'] + \n                                    non_legendary_df['base_experience'] * 0.5)\n\n# Determine combat role\nnon_legendary_df['combat_role'] = non_legendary_df.apply(lambda row:\n    'sweeper' if row['attack'] > row['defense'] and row['speed'] >= 80\n    else 'tank' if row['defense'] > row['attack'] and row['hp'] >= 80\n    else 'balanced',\n    axis=1\n)",
          "execution_time_ms": null,
          "notes": "Non-legendary Pokemon classified by combat effectiveness"
        },
        {
          "stage_id": "merge_and_load",
          "stage_number": 5,
          "stage_name": "Merge & Load",
          "stage_type": "data_loading",
          "description": "Merge both processing branches and load final dataset to PostgreSQL",
          "destination": {
            "table_name": "pokemon_data",
            "write_mode": "replace",
            "create_indexes": true,
            "index_columns": [
              "pokemon_id",
              "name",
              "type_primary",
              "rarity",
              "is_legendary"
            ]
          },
          "code_snippet": "# Ensure schema compatibility\nif 'combat_role' not in legendary_df.columns:\n    legendary_df['combat_role'] = None\n\n# Merge branches\ndf = pd.concat([legendary_df, non_legendary_df], ignore_index=True)\n\n# Load to database\nfrom sqlalchemy import create_engine\nengine = create_engine(os.environ['AIVEN_PG_URI'])\ndf.to_sql('pokemon_data', engine, if_exists='replace', index=False)\n\n# Create indexes\nwith engine.connect() as conn:\n    for col in ['pokemon_id', 'name', 'type_primary', 'rarity']:\n        conn.execute(text(f'CREATE INDEX idx_{col} ON pokemon_data({col})'))",
          "execution_time_ms": null,
          "notes": "Both processing paths merged and loaded to database with optimized indexes"
        }
      ],
      "total_execution_time_ms": null,
      "final_output": {
        "database_table": "pokemon_data",
        "record_count": null
      }
    },
    {
      "pipeline_id": "spacex_launches",
      "pipeline_name": "SpaceX Launch Analytics",
      "description": "Advanced ETL pipeline analyzing SpaceX launches with multi-source integration, data quality branching, and enrichment",
      "source_type": "api",
      "status": "active",
      "last_run": null,
      "detailed_explanation": {
        "overview": "This pipeline pulls launch, rocket, and launchpad info and puts them together. If a launch has enough details we add a few extra fields; if not we keep a simpler version. You learn how to merge sources and handle missing pieces.",
        "what_you_learn": [
          "Joining data from multiple API sources",
          "Implementing data quality checks and routing",
          "Building reliability metrics from historical data"
        ],
        "stage_details": [
          {
            "stage_number": 1,
            "what_happens": "We call three endpoints: launches, rockets, launchpads. Each returns its own JSON list.",
            "why_it_matters": "Data often lives in separate places. You must pull pieces before combining them.",
            "technical_note": "Simple API GET calls; store raw lists for later matching."
          },
          {
            "stage_number": 2,
            "what_happens": "We match each launch to its rocket and launchpad using IDs so all info sits in one row.",
            "why_it_matters": "Having everything together makes filtering and viewing launches easier.",
            "technical_note": "Basic join by ID; keep launches even when related data is missing."
          },
          {
            "stage_number": 3,
            "what_happens": "We check if a launch has key fields. Good ones get extra fields; missing ones stay simple.",
            "why_it_matters": "Treating complete and incomplete data differently avoids errors and still keeps partial records.",
            "technical_note": "Score = fraction of required fields present. Threshold decides path."
          },
          {
            "stage_number": 4,
            "what_happens": "We stack both sets back together so there is one final list of launches.",
            "why_it_matters": "A single table is easier to query and show in the app.",
            "technical_note": "Simple concat; missing advanced fields remain blank."
          },
          {
            "stage_number": 5,
            "what_happens": "We save the final launches table to the database for quick lookups.",
            "why_it_matters": "Storing data lets you query past launches without re-calling the API.",
            "technical_note": "Basic write + a few indexes on common filter columns."
          }
        ]
      },
      "stages": [
        {
          "stage_id": "extract_spacex_data",
          "stage_number": 1,
          "stage_name": "Extract Multi-Source Data",
          "stage_type": "data_ingestion",
          "description": "Fetch launch, rocket, and launchpad data from SpaceX API v5 endpoints",
          "source": {
            "type": "api",
            "base_url": "https://api.spacexdata.com/v5",
            "endpoints": ["launches/past", "rockets", "launchpads"]
          },
          "output": {
            "format": "dataframes",
            "tables": ["launches", "rockets", "launchpads"]
          },
          "code_snippet": "import requests\nimport pandas as pd\n\nbase_url = 'https://api.spacexdata.com/v5'\n\n# Fetch launches\nresponse = requests.get(f'{base_url}/launches/past')\nlaunches_data = response.json()[-100:]  # Last 100 launches\n\n# Fetch rockets\nrockets_data = requests.get(f'{base_url}/rockets').json()\n\n# Fetch launchpads\nlaunchpads_data = requests.get(f'{base_url}/launchpads').json()",
          "execution_time_ms": null,
          "notes": "Extracting from three different endpoints to build comprehensive launch analytics dataset"
        },
        {
          "stage_id": "enrich_and_join",
          "stage_number": 2,
          "stage_name": "Enrich & Join Data Sources",
          "stage_type": "data_transformation",
          "description": "Join launches with rocket and launchpad metadata, calculate derived metrics",
          "transformations": [
            {
              "operation": "join_tables",
              "description": "Left join launches with rockets on rocket_id, then with launchpads on launchpad_id"
            },
            {
              "operation": "datetime_features",
              "description": "Extract year, month, day of week from launch dates"
            },
            {
              "operation": "calculate_efficiency",
              "description": "Calculate cost per payload metric"
            }
          ],
          "code_snippet": "# Join data sources\nenriched_df = launches_df.merge(\n    rockets_df, on='rocket_id', how='left'\n).merge(\n    launchpads_df, on='launchpad_id', how='left'\n)\n\n# Extract date features\nenriched_df['launch_year'] = pd.to_datetime(enriched_df['date_utc']).dt.year\nenriched_df['launch_day_of_week'] = pd.to_datetime(enriched_df['date_utc']).dt.day_name()\n\n# Calculate efficiency\nenriched_df['cost_per_payload'] = enriched_df['cost_per_launch'] / enriched_df['payloads']",
          "execution_time_ms": null,
          "notes": "Enriched dataset combines launch details with rocket specifications and launchpad information"
        },
        {
          "stage_id": "data_quality_branch",
          "stage_number": 3,
          "stage_name": "Data Quality Assessment",
          "stage_type": "data_branching",
          "description": "Evaluate data completeness and route records to appropriate processing paths",
          "branch_condition": "completeness_score >= 0.8",
          "branches": [
            {
              "branch_id": "complete_data_path",
              "condition": "completeness_score >= 80%",
              "next_stage": "process_complete_data"
            },
            {
              "branch_id": "incomplete_data_path",
              "condition": "completeness_score < 80%",
              "next_stage": "process_incomplete_data"
            }
          ],
          "code_snippet": "# Calculate completeness score\nrequired_fields = ['success', 'rocket_name', 'launchpad_name', 'cost_per_launch', 'details']\ndf['completeness_score'] = df[required_fields].notna().sum(axis=1) / len(required_fields)\n\n# Branch data\ncomplete_data = df[df['completeness_score'] >= 0.8]\nincomplete_data = df[df['completeness_score'] < 0.8]\n\nprint(f'Complete: {len(complete_data)}, Incomplete: {len(incomplete_data)}')",
          "execution_time_ms": null,
          "notes": "Data quality branching enables different processing strategies based on data completeness"
        },
        {
          "stage_id": "process_complete_data",
          "stage_number": 4,
          "stage_name": "Advanced Analytics Path",
          "stage_type": "data_transformation",
          "description": "Apply comprehensive transformations and analytics to complete data records",
          "branch_path": "complete",
          "transformations": [
            {
              "operation": "calculate_reliability",
              "description": "Compute reliability score combining success rate and completeness"
            },
            {
              "operation": "classify_complexity",
              "description": "Categorize missions as Low/Medium/High complexity based on crew and payloads"
            },
            {
              "operation": "temporal_features",
              "description": "Calculate days since launch and mission duration metrics"
            }
          ],
          "code_snippet": "# Calculate reliability score\ndf['reliability_score'] = (df['success_rate'] / 100) * df['completeness_score']\n\n# Mission complexity classification\ndf['mission_complexity'] = df.apply(lambda row:\n    'High' if row['crew'] > 0 or row['payloads'] > 3\n    else 'Medium' if row['payloads'] > 1\n    else 'Low',\n    axis=1\n)\n\n# Temporal calculations\ndf['days_since_launch'] = (datetime.now() - df['date_utc']).dt.days",
          "execution_time_ms": null,
          "notes": "Complete data receives full analytical treatment with advanced metrics"
        },
        {
          "stage_id": "process_incomplete_data",
          "stage_number": 4,
          "stage_name": "Basic Processing Path",
          "stage_type": "data_transformation",
          "description": "Apply essential transformations to incomplete data records",
          "branch_path": "incomplete",
          "transformations": [
            {
              "operation": "basic_classification",
              "description": "Apply simple success/failure categorization"
            },
            {
              "operation": "simplified_scoring",
              "description": "Use completeness score as proxy for reliability"
            }
          ],
          "code_snippet": "# Basic mission outcome\ndf['mission_outcome'] = df['success'].apply(\n    lambda x: 'Success' if x == True else 'Failure' if x == False else 'Unknown'\n)\n\n# Simplified reliability\ndf['reliability_score'] = df['completeness_score']\ndf['mission_complexity'] = 'Unknown'",
          "execution_time_ms": null,
          "notes": "Incomplete data receives basic processing to preserve data integrity"
        },
        {
          "stage_id": "merge_enrich_load",
          "stage_number": 5,
          "stage_name": "Merge, Final Enrichment & Load",
          "stage_type": "data_loading",
          "description": "Merge processing branches, add aggregate statistics, and load to PostgreSQL",
          "destination": {
            "table_name": "spacex_launch_analytics",
            "write_mode": "replace",
            "create_indexes": true,
            "index_columns": [
              "flight_number",
              "rocket_name",
              "launchpad_name",
              "launch_year",
              "mission_outcome",
              "processing_tier"
            ]
          },
          "transformations": [
            {
              "operation": "merge_branches",
              "description": "Combine complete and incomplete data processing results"
            },
            {
              "operation": "aggregate_statistics",
              "description": "Calculate rocket-level and launchpad-level success rates"
            }
          ],
          "code_snippet": "# Merge branches\nfinal_df = pd.concat([complete_df, incomplete_df], ignore_index=True)\n\n# Add aggregate statistics\nfinal_df['avg_success_rate_by_rocket'] = final_df.groupby('rocket_name')['success'].transform('mean')\nfinal_df['launches_by_rocket'] = final_df.groupby('rocket_name')['rocket_name'].transform('count')\nfinal_df['avg_success_rate_by_pad'] = final_df.groupby('launchpad_name')['success'].transform('mean')\n\n# Load to database\nfrom sqlalchemy import create_engine\nengine = create_engine(os.environ['AIVEN_PG_URI'])\nfinal_df.to_sql('spacex_launch_analytics', engine, if_exists='replace', index=False)",
          "execution_time_ms": null,
          "notes": "Final dataset includes both processing paths with aggregate statistics for analysis"
        }
      ],
      "total_execution_time_ms": null,
      "final_output": {
        "database_table": "spacex_launch_analytics",
        "record_count": null
      }
    },
    {
      "pipeline_id": "weather_analytics",
      "pipeline_name": "Multi-Region Weather Analytics",
      "description": "Fan-out/Fan-in pipeline fetching weather data from multiple regions in parallel, demonstrating concurrent processing",
      "source_type": "api",
      "status": "active",
      "last_run": null,
      "detailed_explanation": {
        "overview": "This pipeline grabs weather for three cities at the same time, cleans each set, then combines them. You learn simple parallel fetching and how to compare regions.",
        "what_you_learn": [
          "Parallel data extraction for improved performance",
          "Region-specific transformations on partitioned data",
          "Aggregating parallel results into unified datasets"
        ],
        "stage_details": [
          {"stage_number": 1, "what_happens": "Start three API calls for New York, London, and Tokyo.", "why_it_matters": "Doing them together saves time.", "technical_note": "Fire off the requests in parallel."},
          {"stage_number": 2, "what_happens": "Clean each city's rows and add simple extra fields (like comfort info).", "why_it_matters": "Turns raw numbers into easier facts.", "technical_note": "One small function per city."},
          {"stage_number": 3, "what_happens": "Finish the last city fetch.", "why_it_matters": "Now all data is ready to join.", "technical_note": "Just wait for remaining request."},
          {"stage_number": 4, "what_happens": "Confirm all three sets are in memory.", "why_it_matters": "We can combine safely.", "technical_note": "Simple completion check."},
          {"stage_number": 5, "what_happens": "Stack all rows into one table with a region column.", "why_it_matters": "Lets you compare cities easily.", "technical_note": "Simple table append."},
          {"stage_number": 6, "what_happens": "Make daily summaries and simple labels like hot, cold, rainy.", "why_it_matters": "Daily view is easier to digest.", "technical_note": "Group by date and region; add labels by thresholds."},
          {"stage_number": 7, "what_happens": "Save the final table so the app can read it.", "why_it_matters": "Lets you view and reuse the results later.", "technical_note": "Write table + add a few helpful indexes."}
        ]
      },
      "stages": [
        {
          "stage_id": "initiate_parallel_fetch",
          "stage_number": 1,
          "stage_name": "Initialize Parallel Extraction",
          "stage_type": "data_ingestion",
          "description": "Set up parallel fetch configuration for multiple geographic regions",
          "source": {
            "type": "api",
            "base_url": "https://api.open-meteo.com/v1/forecast",
            "regions": [
              {
                "name": "North America",
                "city": "New York",
                "latitude": 40.7128,
                "longitude": -74.0060
              },
              {
                "name": "Europe",
                "city": "London",
                "latitude": 51.5074,
                "longitude": -0.1278
              },
              {
                "name": "Asia",
                "city": "Tokyo",
                "latitude": 35.6762,
                "longitude": 139.6503
              }
            ],
            "parameters": "temperature_2m,relative_humidity_2m,precipitation,wind_speed_10m"
          },
          "output": {
            "format": "configuration",
            "parallel_tasks": 3
          },
          "code_snippet": "import requests\nimport pandas as pd\nfrom concurrent.futures import ThreadPoolExecutor\n\n# Configuration for parallel fetching\nregions = [\n    {'name': 'North America', 'city': 'New York', 'lat': 40.7128, 'lon': -74.0060},\n    {'name': 'Europe', 'city': 'London', 'lat': 51.5074, 'lon': -0.1278},\n    {'name': 'Asia', 'city': 'Tokyo', 'lat': 35.6762, 'lon': 139.6503}\n]",
          "execution_time_ms": null,
          "notes": "Prepares configuration for concurrent API requests to multiple regions"
        },
        {
          "stage_id": "fetch_north_america",
          "stage_number": 2,
          "stage_name": "Fetch North America Data",
          "stage_type": "data_ingestion",
          "description": "Extract weather data for North America region (New York) in parallel",
          "parallel_path": "north_america",
          "output": {
            "format": "dataframe",
            "region": "North America"
          },
          "code_snippet": "# Parallel fetch for North America\nurl = f'https://api.open-meteo.com/v1/forecast?latitude=40.7128&longitude=-74.0060&hourly=temperature_2m,relative_humidity_2m,precipitation,wind_speed_10m&past_days=92'\nresponse = requests.get(url)\ndata = response.json()\ndf_na = pd.DataFrame(data['hourly'])\ndf_na['region'] = 'North America'\ndf_na['city'] = 'New York'",
          "execution_time_ms": null,
          "notes": "Runs concurrently with other region fetches using ThreadPoolExecutor"
        },
        {
          "stage_id": "fetch_europe",
          "stage_number": 3,
          "stage_name": "Fetch Europe Data",
          "stage_type": "data_ingestion",
          "description": "Extract weather data for Europe region (London) in parallel",
          "parallel_path": "europe",
          "output": {
            "format": "dataframe",
            "region": "Europe"
          },
          "code_snippet": "# Parallel fetch for Europe\nurl = f'https://api.open-meteo.com/v1/forecast?latitude=51.5074&longitude=-0.1278&hourly=temperature_2m,relative_humidity_2m,precipitation,wind_speed_10m&past_days=92'\nresponse = requests.get(url)\ndata = response.json()\ndf_eu = pd.DataFrame(data['hourly'])\ndf_eu['region'] = 'Europe'\ndf_eu['city'] = 'London'",
          "execution_time_ms": null,
          "notes": "Runs concurrently with other region fetches using ThreadPoolExecutor"
        },
        {
          "stage_id": "fetch_asia",
          "stage_number": 4,
          "stage_name": "Fetch Asia Data",
          "stage_type": "data_ingestion",
          "description": "Extract weather data for Asia region (Tokyo) in parallel",
          "parallel_path": "asia",
          "output": {
            "format": "dataframe",
            "region": "Asia"
          },
          "code_snippet": "# Parallel fetch for Asia\nurl = f'https://api.open-meteo.com/v1/forecast?latitude=35.6762&longitude=139.6503&hourly=temperature_2m,relative_humidity_2m,precipitation,wind_speed_10m&past_days=92'\nresponse = requests.get(url)\ndata = response.json()\ndf_asia = pd.DataFrame(data['hourly'])\ndf_asia['region'] = 'Asia'\ndf_asia['city'] = 'Tokyo'",
          "execution_time_ms": null,
          "notes": "Runs concurrently with other region fetches using ThreadPoolExecutor"
        },
        {
          "stage_id": "merge_regional_data",
          "stage_number": 5,
          "stage_name": "Merge Regional Data (Fan-in)",
          "stage_type": "data_transformation",
          "description": "Combine all regional weather data into unified dataset",
          "transformations": [
            {
              "operation": "concatenate_dataframes",
              "description": "Merge North America, Europe, and Asia datasets"
            },
            {
              "operation": "standardize_timestamps",
              "description": "Convert all timestamps to UTC and parse datetime"
            }
          ],
          "output": {
            "format": "dataframe",
            "total_regions": 3
          },
          "code_snippet": "# Fan-in: Merge all regional data\nmerged_df = pd.concat([df_na, df_eu, df_asia], ignore_index=True)\n\n# Standardize timestamps\nmerged_df['time'] = pd.to_datetime(merged_df['time'])\nmerged_df['date'] = merged_df['time'].dt.date\nmerged_df['hour'] = merged_df['time'].dt.hour",
          "execution_time_ms": null,
          "notes": "Fan-in point where parallel streams converge into single dataset"
        },
        {
          "stage_id": "transform_weather_data",
          "stage_number": 6,
          "stage_name": "Calculate Analytics",
          "stage_type": "data_transformation",
          "description": "Calculate aggregate statistics and comparative metrics across regions",
          "transformations": [
            {
              "operation": "calculate_daily_aggregates",
              "description": "Compute daily min, max, average temperatures per region"
            },
            {
              "operation": "weather_classification",
              "description": "Classify weather conditions (hot, moderate, cold, rainy)"
            },
            {
              "operation": "regional_comparisons",
              "description": "Calculate temperature differentials between regions"
            }
          ],
          "code_snippet": "# Calculate daily aggregates\ndaily_stats = merged_df.groupby(['region', 'city', 'date']).agg({\n    'temperature_2m': ['min', 'max', 'mean'],\n    'precipitation': 'sum',\n    'wind_speed_10m': 'mean',\n    'relative_humidity_2m': 'mean'\n}).reset_index()\n\n# Weather classification\ndf['weather_type'] = df.apply(lambda row:\n    'rainy' if row['precipitation'] > 5\n    else 'hot' if row['temperature_2m'] > 25\n    else 'cold' if row['temperature_2m'] < 5\n    else 'moderate',\n    axis=1\n)",
          "execution_time_ms": null,
          "notes": "Enriches merged dataset with calculated metrics for analysis"
        },
        {
          "stage_id": "load_weather_data",
          "stage_number": 7,
          "stage_name": "Load to PostgreSQL",
          "stage_type": "data_loading",
          "description": "Load processed multi-region weather analytics to database",
          "destination": {
            "table_name": "weather_analytics",
            "write_mode": "replace",
            "create_indexes": true,
            "index_columns": [
              "region",
              "city",
              "time",
              "date"
            ]
          },
          "code_snippet": "from sqlalchemy import create_engine\nimport os\n\n# Create engine\nengine = create_engine(os.environ['AIVEN_PG_URI'])\n\n# Drop existing table\nwith engine.connect() as conn:\n    conn.execute(text('DROP TABLE IF EXISTS weather_analytics CASCADE'))\n    conn.commit()\n\n# Load data\ndf.to_sql('weather_analytics', engine, if_exists='replace', index=False)\n\n# Create indexes\nwith engine.connect() as conn:\n    for col in ['region', 'city', 'date']:\n        conn.execute(text(f'CREATE INDEX IF NOT EXISTS idx_weather_{col} ON weather_analytics({col})'))\n    conn.commit()",
          "execution_time_ms": null,
          "notes": "Final dataset contains weather data from all regions with analytics"
        }
      ],
      "total_execution_time_ms": null,
      "final_output": {
        "database_table": "weather_analytics",
        "record_count": null
      }
    },
    {
      "pipeline_id": "hackernews_scraper",
      "pipeline_name": "Hacker News Web Scraper",
      "description": "Web scraping pipeline extracting front page posts from Hacker News, demonstrating data collection from public websites",
      "source_type": "web_scraping",
      "status": "active",
      "last_run": null,
      "detailed_explanation": {
        "overview": "This beginner web scraping pipeline extracts article data directly from HTML pages. It demonstrates how to gather data from websites that don't provide APIs, parsing structure from raw HTML.",
        "what_you_learn": [
          "HTML parsing and web scraping with BeautifulSoup",
          "Ethical scraping practices (delays, user agents)",
          "Extracting and structuring unformatted web data"
        ],
        "stage_details": [
          {
            "stage_number": 1,
            "what_happens": "The pipeline requests 7 pages from Hacker News, extracting titles, URLs, points, authors, and comment counts from the HTML structure. It includes polite delays between requests to avoid overloading the server.",
            "why_it_matters": "Many valuable data sources lack APIs. Web scraping unlocks this data, but must be done responsibly to respect server resources and terms of service.",
            "technical_note": "Uses BeautifulSoup to parse HTML and find elements by class name, with time.sleep() for rate limiting."
          },
          {
            "stage_number": 2,
            "what_happens": "Raw scraped data gets cleaned and enriched. The pipeline calculates engagement scores (points + comments), categorizes popularity (viral, popular, moderate, new), and standardizes text fields.",
            "why_it_matters": "Scraped data is often messy with inconsistent formats. Transformations create structured, analysis-ready datasets from unstructured HTML.",
            "technical_note": "Uses string parsing, numerical calculations, and conditional categorization to derive meaningful metrics."
          },
          {
            "stage_number": 3,
            "what_happens": "Cleaned article data loads into PostgreSQL. Each post becomes a searchable database record with metadata like engagement scores and popularity categories.",
            "why_it_matters": "Storing scraped data enables trend analysis over time, automated alerts for trending topics, and integration with other data sources.",
            "technical_note": "Uses pandas to_sql() with if_exists='replace' to refresh the dataset with latest trending posts."
          }
        ]
      },
      "stages": [
        {
          "stage_id": "scrape_hackernews",
          "stage_number": 1,
          "stage_name": "Scrape Front Page",
          "stage_type": "data_ingestion",
          "description": "Extract posts from Hacker News front page using BeautifulSoup web scraping",
          "source": {
            "type": "web_scraping",
            "base_url": "https://news.ycombinator.com",
            "method": "requests + BeautifulSoup",
            "pages": 7
          },
          "output": {
            "format": "list",
            "fields": [
              "story_id",
              "title",
              "url",
              "points",
              "author",
              "age",
              "comments",
              "source",
              "scraped_at"
            ]
          },
          "code_snippet": "import requests\nfrom bs4 import BeautifulSoup\n\nposts = []\nfor page in range(1, 8):\n    url = f'https://news.ycombinator.com/?p={page}'\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    story_rows = soup.select('tr.athing')\n    for story in story_rows:\n        title = story.select_one('.titleline > a').text\n        url_link = story.select_one('.titleline > a')['href']\n        # Extract points, author, comments from subtext...\n        posts.append({...})",
          "execution_time_ms": null,
          "notes": "Scrapes ~200 posts from Hacker News with polite delay between requests"
        },
        {
          "stage_id": "transform_posts",
          "stage_number": 2,
          "stage_name": "Transform & Enrich",
          "stage_type": "data_transformation",
          "description": "Clean URLs, calculate engagement metrics, and categorize post popularity",
          "transformations": [
            {
              "operation": "calculate_engagement",
              "description": "Sum points and comments to create engagement_score metric"
            },
            {
              "operation": "categorize_popularity",
              "description": "Classify posts as viral (100+), popular (50+), moderate (20+), or new (<20)"
            },
            {
              "operation": "clean_urls",
              "description": "Convert relative URLs to absolute, flag external vs internal links"
            },
            {
              "operation": "limit_rows",
              "description": "Limit to 200 posts for consistent dataset size"
            }
          ],
          "output": {
            "format": "dataframe",
            "columns": [
              "story_id",
              "title",
              "url",
              "points",
              "author",
              "age",
              "comments",
              "source",
              "scraped_at",
              "engagement_score",
              "popularity",
              "is_external"
            ]
          },
          "code_snippet": "df = pd.DataFrame(posts).head(200)\n\n# Calculate engagement\ndf['engagement_score'] = df['points'] + df['comments']\n\n# Categorize popularity\ndef categorize(row):\n    score = row['engagement_score']\n    if score >= 100: return 'viral'\n    elif score >= 50: return 'popular'\n    elif score >= 20: return 'moderate'\n    else: return 'new'\n\ndf['popularity'] = df.apply(categorize, axis=1)\ndf['is_external'] = ~df['url'].str.contains('news.ycombinator.com')",
          "execution_time_ms": null,
          "notes": "Enriches raw scraped data with computed metrics"
        },
        {
          "stage_id": "load_posts",
          "stage_number": 3,
          "stage_name": "Load to PostgreSQL",
          "stage_type": "data_loading",
          "description": "Load scraped and transformed Hacker News posts to database",
          "destination": {
            "table_name": "hackernews_posts",
            "write_mode": "replace",
            "create_indexes": true,
            "index_columns": [
              "story_id",
              "author",
              "popularity",
              "scraped_at"
            ]
          },
          "code_snippet": "from sqlalchemy import create_engine\nimport os\n\nengine = create_engine(os.environ['AIVEN_PG_URI'])\n\n# Drop existing table\nwith engine.connect() as conn:\n    conn.execute(text('DROP TABLE IF EXISTS hackernews_posts CASCADE'))\n    conn.commit()\n\n# Load data\ndf.to_sql('hackernews_posts', engine, if_exists='replace', index=False)\n\n# Create indexes\nwith engine.connect() as conn:\n    for col in ['story_id', 'author', 'popularity', 'scraped_at']:\n        conn.execute(text(f'CREATE INDEX IF NOT EXISTS idx_hn_{col} ON hackernews_posts({col})'))\n    conn.commit()",
          "execution_time_ms": null,
          "notes": "Final dataset contains web-scraped Hacker News posts with engagement metrics"
        }
      ],
      "total_execution_time_ms": null,
      "final_output": {
        "database_table": "hackernews_posts",
        "record_count": null
      }
    },
    {
      "pipeline_id": "network_traffic",
      "pipeline_name": "Network Traffic Anomaly Detection",
      "description": "Intermediate security analytics pipeline analyzing network traffic patterns, detecting anomalies, and calculating risk scores",
      "source_type": "file",
      "status": "active",
      "last_run": null,
      "detailed_explanation": {
        "overview": "This pipeline reads a small network traffic sample and gives each row a simple risk score. You learn how to spot basic patterns like unusual ports or very small packets and turn them into a priority flag.",
        "what_you_learn": [
          "Working with security datasets from Kaggle",
          "Building risk scoring systems with multiple factors",
          "Identifying anomalies using pattern analysis"
        ],
        "stage_details": [
          {"stage_number": 1, "what_happens": "Load a small CSV of network packets (IPs, ports, protocol, size).", "why_it_matters": "Gives raw rows to classify.", "technical_note": "Simple dataset download + limit to first 200 rows."},
          {"stage_number": 2, "what_happens": "Classify protocol type and flag packets using common risky ports.", "why_it_matters": "Helps surface traffic worth a closer look.", "technical_note": "Check port lists; simple size bucket labels."},
          {"stage_number": 3, "what_happens": "Add points for suspicious signals (risky port, tiny packet, very large packet, heavy traffic).", "why_it_matters": "Creates a single number to sort by urgency.", "technical_note": "Add fixed points; convert total into low/medium/high/critical."},
          {"stage_number": 4, "what_happens": "Save scored rows to the database.", "why_it_matters": "Lets the app show and filter by threat level.", "technical_note": "Simple table write + indexes for common filters."}
        ]
      },
      "stages": [
        {
          "stage_id": "extract_kaggle_traffic",
          "stage_number": 1,
          "stage_name": "Extract Network Traffic Data",
          "stage_type": "data_ingestion",
          "description": "Download network traffic dataset from Kaggle using KaggleHub",
          "source": {
            "type": "kagglehub",
            "dataset_id": "ziya07/network-traffic-anomaly-detection-dataset",
            "file_format": "csv"
          },
          "output": {
            "format": "dataframe",
            "columns": [
              "Timestamp",
              "Source IP",
              "Destination IP",
              "Source Port",
              "Destination Port",
              "Protocol",
              "Packet Length",
              "Payload Data"
            ]
          },
          "code_snippet": "import kagglehub\nfrom kagglehub import KaggleDatasetAdapter\n\n# Download dataset\ndf = kagglehub.load_dataset(\n    KaggleDatasetAdapter.PANDAS,\n    'ziya07/network-traffic-anomaly-detection-dataset',\n    '',\n)\n\n# Limit to 200 records\ndf = df.head(200)",
          "execution_time_ms": null,
          "notes": "Network traffic data with packet-level details for security analysis"
        },
        {
          "stage_id": "analyze_traffic_patterns",
          "stage_number": 2,
          "stage_name": "Analyze Traffic Patterns",
          "stage_type": "data_transformation",
          "description": "Classify protocols, detect suspicious ports, analyze traffic direction and packet characteristics",
          "transformations": [
            {
              "operation": "classify_protocol",
              "description": "Categorize network protocols (TCP, UDP, ICMP, HTTP, Other)"
            },
            {
              "operation": "categorize_packet_size",
              "description": "Classify packets as tiny/small/medium/large based on length"
            },
            {
              "operation": "detect_suspicious_ports",
              "description": "Flag traffic using common attack vector ports (Telnet, RPC, SMB, RDP, VNC)"
            },
            {
              "operation": "analyze_traffic_direction",
              "description": "Determine if traffic is internal, inbound, or outbound based on IP addresses"
            },
            {
              "operation": "calculate_traffic_intensity",
              "description": "Rolling average of packet sizes to detect traffic bursts"
            }
          ],
          "output": {
            "format": "dataframe",
            "new_columns": [
              "protocol_type",
              "packet_category",
              "uses_suspicious_port",
              "traffic_direction",
              "traffic_intensity",
              "payload_size",
              "has_payload"
            ]
          },
          "code_snippet": "# Classify protocol\ndf['protocol_type'] = df['Protocol'].apply(classify_protocol)\n\n# Categorize packet size\ndf['packet_category'] = pd.cut(df['Packet Length'], \n    bins=[0, 100, 500, 1500, float('inf')],\n    labels=['tiny', 'small', 'medium', 'large'])\n\n# Detect suspicious ports\nsuspicious_ports = [23, 135, 139, 445, 1433, 3389, 5900]\ndf['uses_suspicious_port'] = (\n    df['Source Port'].isin(suspicious_ports) | \n    df['Destination Port'].isin(suspicious_ports)\n)\n\n# Analyze traffic direction\ndf['traffic_direction'] = df.apply(\n    lambda row: 'internal' if is_internal(row['Source IP']) and is_internal(row['Destination IP'])\n    else 'outbound' if is_internal(row['Source IP'])\n    else 'inbound', axis=1\n)",
          "execution_time_ms": null,
          "notes": "Deep packet inspection and traffic pattern analysis for security insights"
        },
        {
          "stage_id": "calculate_risk_scores",
          "stage_number": 3,
          "stage_name": "Calculate Risk Scores",
          "stage_type": "data_transformation",
          "description": "Assign risk points based on security factors and categorize threat levels",
          "transformations": [
            {
              "operation": "risk_scoring",
              "description": "Calculate composite risk score (0-100) based on suspicious indicators",
              "factors": [
                "Suspicious port usage (+30 points)",
                "Unusual packet sizes (+15-20 points)",
                "High traffic intensity (+25 points)",
                "Inbound unknown traffic (+15 points)",
                "No payload/reconnaissance (+10 points)"
              ]
            },
            {
              "operation": "categorize_threat",
              "description": "Classify threat level as low/medium/high/critical based on risk score"
            },
            {
              "operation": "calculate_anomaly_confidence",
              "description": "Percentage confidence (0-100%) that traffic is anomalous"
            },
            {
              "operation": "flag_investigation",
              "description": "Mark high/critical threats for security team review"
            }
          ],
          "output": {
            "format": "dataframe",
            "new_columns": [
              "risk_score",
              "threat_level",
              "anomaly_confidence",
              "requires_investigation",
              "processed_at"
            ]
          },
          "code_snippet": "# Initialize risk score\ndf['risk_score'] = 0\n\n# Add risk points\ndf.loc[df['uses_suspicious_port'], 'risk_score'] += 30\ndf.loc[df['Packet Length'] < 50, 'risk_score'] += 20\ndf.loc[df['Packet Length'] > 1400, 'risk_score'] += 15\n\n# High traffic intensity\nintensity_threshold = df['traffic_intensity'].quantile(0.9)\ndf.loc[df['traffic_intensity'] > intensity_threshold, 'risk_score'] += 25\n\n# Categorize threat level\ndf['threat_level'] = pd.cut(df['risk_score'],\n    bins=[-1, 20, 40, 60, 100],\n    labels=['low', 'medium', 'high', 'critical'])\n\n# Anomaly confidence\ndf['anomaly_confidence'] = (df['risk_score'] / df['risk_score'].max() * 100).round(2)\n\n# Flag for investigation\ndf['requires_investigation'] = df['threat_level'].isin(['high', 'critical'])",
          "execution_time_ms": null,
          "notes": "Multi-factor security risk assessment with actionable threat intelligence"
        },
        {
          "stage_id": "load_traffic_data",
          "stage_number": 4,
          "stage_name": "Load to PostgreSQL",
          "stage_type": "data_loading",
          "description": "Load analyzed network traffic with security metrics to database",
          "destination": {
            "table_name": "network_traffic_analysis",
            "write_mode": "replace",
            "create_indexes": true,
            "index_columns": [
              "threat_level",
              "requires_investigation",
              "protocol_type",
              "traffic_direction"
            ]
          },
          "code_snippet": "from sqlalchemy import create_engine\nimport os\n\nengine = create_engine(os.environ['AIVEN_PG_URI'])\n\n# Drop existing table\nwith engine.connect() as conn:\n    conn.execute(text('DROP TABLE IF EXISTS network_traffic_analysis CASCADE'))\n    conn.commit()\n\n# Load data\ndf.to_sql('network_traffic_analysis', engine, if_exists='replace', index=False)\n\n# Create indexes\nwith engine.connect() as conn:\n    for col in ['threat_level', 'requires_investigation', 'protocol_type', 'traffic_direction']:\n        conn.execute(text(f'CREATE INDEX IF NOT EXISTS idx_traffic_{col} ON network_traffic_analysis({col})'))\n    conn.commit()",
          "execution_time_ms": null,
          "notes": "Security-focused dataset with risk scores, threat categorization, and investigation flags"
        }
      ],
      "total_execution_time_ms": null,
      "final_output": {
        "database_table": "network_traffic_analysis",
        "record_count": null
      }
    },
    {
      "pipeline_id": "stock_market",
      "pipeline_name": "Stock Market Time-Series Analytics",
      "description": "Time-series pipeline analyzing stock prices with technical indicators, moving averages, and trend detection",
      "source_type": "api",
      "status": "active",
      "last_run": null,
      "detailed_explanation": {
        "overview": "This pipeline pulls recent prices for three stocks, adds a few simple calculations like moving averages and basic trend labels, then stores the result. You learn how to work with timeordered rows.",
        "what_you_learn": [
          "Processing time-series data with temporal ordering",
          "Calculating technical indicators (RSI, moving averages, volatility)",
          "Implementing rolling window computations and trend detection"
        ],
        "stage_details": [
          {"stage_number": 1, "what_happens": "Get daily prices for Apple, Google, Microsoft (open, high, low, close, volume).", "why_it_matters": "Price history is the base for all later calculations.", "technical_note": "Simple API call per symbol; turn JSON into rows."},
          {"stage_number": 2, "what_happens": "Add moving averages and simple daily change stats.", "why_it_matters": "Smoothed lines and percent changes make patterns easier to see.", "technical_note": "Basic rolling averages + percent change."},
          {"stage_number": 3, "what_happens": "Label each day (trend up, down, or flat) and simple volume buckets.", "why_it_matters": "Categories are easier to filter and explain than raw numbers.", "technical_note": "Compare price to averages; rough cutoffs for volume."},
          {"stage_number": 4, "what_happens": "Save final rows to the database for quick lookups.", "why_it_matters": "Lets the app show historic trends without refetching.", "technical_note": "Store table + indexes on symbol and date."}
        ]
      },
      "stages": [
        {
          "stage_id": "extract_stock_data",
          "stage_number": 1,
          "stage_name": "Extract Stock Prices",
          "stage_type": "data_ingestion",
          "description": "Fetch 90 days of OHLC stock data for AAPL, GOOGL, MSFT from Yahoo Finance API",
          "source": {
            "type": "api",
            "base_url": "https://query1.finance.yahoo.com/v8/finance/chart",
            "symbols": ["AAPL", "GOOGL", "MSFT"],
            "timeframe": "90 days",
            "interval": "1d"
          },
          "output": {
            "format": "dataframe",
            "columns": [
              "timestamp",
              "symbol",
              "open",
              "high",
              "low",
              "close",
              "volume"
            ]
          },
          "code_snippet": "import requests\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nend_date = datetime.now()\nstart_date = end_date - timedelta(days=90)\n\nfor symbol in ['AAPL', 'GOOGL', 'MSFT']:\n    url = f'https://query1.finance.yahoo.com/v8/finance/chart/{symbol}'\n    params = {\n        'period1': int(start_date.timestamp()),\n        'period2': int(end_date.timestamp()),\n        'interval': '1d'\n    }\n    response = requests.get(url, params=params)\n    data = response.json()",
          "execution_time_ms": null,
          "notes": "Fetches daily OHLC (Open, High, Low, Close) prices with volume for time-series analysis"
        },
        {
          "stage_id": "calculate_indicators",
          "stage_number": 2,
          "stage_name": "Calculate Technical Indicators",
          "stage_type": "data_transformation",
          "description": "Compute moving averages, RSI, volatility, and momentum using rolling windows",
          "transformations": [
            {
              "operation": "daily_returns",
              "description": "Calculate percentage change from previous day's close"
            },
            {
              "operation": "moving_averages",
              "description": "Compute 7-day and 20-day simple moving averages"
            },
            {
              "operation": "rsi_calculation",
              "description": "Calculate 14-day Relative Strength Index (0-100 scale)"
            },
            {
              "operation": "volatility_measure",
              "description": "Compute 7-day rolling standard deviation of returns"
            },
            {
              "operation": "momentum_indicator",
              "description": "Calculate price momentum relative to 7-day MA"
            }
          ],
          "output": {
            "format": "dataframe",
            "new_columns": [
              "daily_return",
              "ma_7",
              "ma_20",
              "volatility_7d",
              "rsi",
              "momentum"
            ]
          },
          "code_snippet": "# Calculate for each stock\ndf = df.sort_values(['symbol', 'timestamp'])\n\n# Daily returns\ndf['daily_return'] = df.groupby('symbol')['close'].pct_change() * 100\n\n# Moving averages\ndf['ma_7'] = df.groupby('symbol')['close'].rolling(7, min_periods=1).mean().reset_index(0, drop=True)\ndf['ma_20'] = df.groupby('symbol')['close'].rolling(20, min_periods=1).mean().reset_index(0, drop=True)\n\n# RSI calculation\ndelta = df.groupby('symbol')['close'].diff()\ngain = delta.where(delta > 0, 0).rolling(14, min_periods=1).mean()\nloss = -delta.where(delta < 0, 0).rolling(14, min_periods=1).mean()\nrs = gain / loss\ndf['rsi'] = 100 - (100 / (1 + rs))",
          "execution_time_ms": null,
          "notes": "Technical indicators use rolling windows - proper sorting by symbol and time is critical"
        },
        {
          "stage_id": "enrich_market_context",
          "stage_number": 3,
          "stage_name": "Add Market Context",
          "stage_type": "data_transformation",
          "description": "Classify trends, volume patterns, RSI signals, and volatility levels",
          "transformations": [
            {
              "operation": "volume_classification",
              "description": "Categorize trading volume as high/normal/low relative to stock average"
            },
            {
              "operation": "trend_detection",
              "description": "Classify as bullish/bearish/neutral based on MA relationships"
            },
            {
              "operation": "rsi_signals",
              "description": "Flag overbought (>70) and oversold (<30) conditions"
            },
            {
              "operation": "volatility_buckets",
              "description": "Classify volatility as high/medium/low using quantiles"
            },
            {
              "operation": "day_type_classification",
              "description": "Categorize trading days by price change magnitude"
            }
          ],
          "output": {
            "format": "dataframe",
            "new_columns": [
              "volume_category",
              "trend",
              "rsi_signal",
              "volatility_level",
              "price_change",
              "price_change_pct",
              "day_type"
            ]
          },
          "code_snippet": "# Trend classification\ndf['trend'] = df.apply(\n    lambda row: 'bullish' if row['close'] > row['ma_20'] and row['ma_7'] > row['ma_20']\n    else 'bearish' if row['close'] < row['ma_20'] and row['ma_7'] < row['ma_20']\n    else 'neutral',\n    axis=1\n)\n\n# RSI signals\ndf['rsi_signal'] = df['rsi'].apply(\n    lambda x: 'overbought' if x > 70\n    else 'oversold' if x < 30\n    else 'neutral'\n)\n\n# Day type\ndf['day_type'] = df['price_change_pct'].apply(\n    lambda x: 'strong_gain' if x > 2 else 'gain' if x > 0 else 'strong_loss' if x < -2 else 'loss'\n)",
          "execution_time_ms": null,
          "notes": "Converts continuous metrics into actionable categorical signals for filtering and alerts"
        },
        {
          "stage_id": "load_stock_analytics",
          "stage_number": 4,
          "stage_name": "Load to PostgreSQL",
          "stage_type": "data_loading",
          "description": "Load time-series stock data with specialized temporal indexes",
          "destination": {
            "table_name": "stock_market_analytics",
            "write_mode": "replace",
            "create_indexes": true,
            "index_columns": [
              "timestamp",
              "symbol",
              "date",
              "symbol, date (composite)"
            ]
          },
          "code_snippet": "from sqlalchemy import create_engine, text\nimport os\n\nengine = create_engine(os.environ['AIVEN_PG_URI'])\n\n# Drop existing table\nwith engine.connect() as conn:\n    conn.execute(text('DROP TABLE IF EXISTS stock_market_analytics CASCADE'))\n    conn.commit()\n\n# Load data\ndf.to_sql('stock_market_analytics', engine, if_exists='replace', index=False)\n\n# Create time-series optimized indexes\nwith engine.connect() as conn:\n    conn.execute(text('CREATE INDEX idx_stock_timestamp ON stock_market_analytics(timestamp)'))\n    conn.execute(text('CREATE INDEX idx_stock_symbol ON stock_market_analytics(symbol)'))\n    conn.execute(text('CREATE INDEX idx_stock_date ON stock_market_analytics(date)'))\n    conn.execute(text('CREATE INDEX idx_stock_symbol_date ON stock_market_analytics(symbol, date)'))\n    conn.commit()",
          "execution_time_ms": null,
          "notes": "Composite index on (symbol, date) dramatically speeds up stock-specific time-range queries"
        }
      ],
      "total_execution_time_ms": null,
      "final_output": {
        "database_table": "stock_market_analytics",
        "record_count": null
      }
    }
    ,
    {
      "pipeline_id": "shipping_disruptions",
      "pipeline_name": "Global Shipping Disruption Monitoring",
      "description": "Geospatial/event detection pipeline computing port disruption risk scores from vessel positions",
      "source_type": "file",
      "status": "active",
      "last_run": null,
      "detailed_explanation": {
        "overview": "This pipeline looks at a tiny sample of ship positions and gives each port a simple congestion score. We count how many ships are sitting still, how many appear to be waiting, and how many are moving. From that we make an easy-to-read risk score.",
        "what_you_learn": [
          "How raw position rows become port activity",
          "Identifying waiting vs moving ships with speed",
          "Turning counts into a simple risk score"
        ],
        "stage_details": [
          {"stage_number": 1, "what_happens": "Load a small CSV of ship positions (id, location, speed, time).", "why_it_matters": "Gives us basic movement snapshots to work with.", "technical_note": "Tiny file keeps things fast and low storage."},
          {"stage_number": 2, "what_happens": "Match each position to a port area or its waiting area (anchorage).", "why_it_matters": "Lets us tell whether a ship is inside the port or waiting nearby.", "technical_note": "Just simple box checks (min/max lat & lon)."},
          {"stage_number": 3, "what_happens": "Mark slow ships in the waiting area as \"dwell\" and fast ships inside the port as \"throughput\".", "why_it_matters": "Separates idle queue buildup from active movement.", "technical_note": "Uses two speed cutoffs: <=0.5 = still, >=5 = moving."},
          {"stage_number": 4, "what_happens": "Count dwell events, unique waiting ships, and moving ships; blend into a 0100 score.", "why_it_matters": "A single number is easy to scan for rising congestion.", "technical_note": "Weighted mix: waiting + dwell heavier than movement drop."},
          {"stage_number": 5, "what_happens": "Save scores to the database so the app can show them.", "why_it_matters": "Persists results for viewing and future comparison.", "technical_note": "Stores ports and scores tables; adds basic indexes."}
        ]
      },
      "stages": [
        {"stage_id": "ingest_positions", "stage_number": 1, "stage_name": "Ingest Positions", "stage_type": "data_ingestion", "description": "Load AIS-like vessel positions from CSV", "source": {"type": "file", "path": "backend/data/ais_sample.csv"}, "output": {"format": "rows", "columns": ["vessel_id", "lat", "lon", "speed_knots", "timestamp"]}, "code_snippet": "import csv\nwith open('backend/data/ais_sample.csv', 'r', encoding='utf-8') as f:\n    reader = csv.DictReader(f)\n    positions = list(reader)"},
        {"stage_id": "geofence_join", "stage_number": 2, "stage_name": "Geofence Join", "stage_type": "data_transformation", "description": "Tag positions to ports/anchorages via bbox checks", "output": {"format": "rows", "columns": ["port_id", "location_type"]}, "code_snippet": "def in_box(lat, lon, box):\n    return box['min_lat'] <= lat <= box['max_lat'] and box['min_lon'] <= lon <= box['max_lon']\n# Iterate positions, assign port_id/location_type from configured boxes"},
        {"stage_id": "event_extraction", "stage_number": 3, "stage_name": "Extract Events", "stage_type": "data_transformation", "description": "Classify dwell and moving events using speed thresholds", "output": {"format": "rows", "columns": ["event_type", "port_id", "vessel_id", "timestamp"]}, "code_snippet": "if location_type == 'anchorage' and speed_knots <= 0.5:\n    event = 'dwell'\nelif location_type == 'port' and speed_knots >= 5:\n    event = 'moving'"},
        {"stage_id": "compute_scores", "stage_number": 4, "stage_name": "Compute Scores", "stage_type": "data_transformation", "description": "Aggregate metrics per port and compute risk scores", "output": {"format": "table", "columns": ["port_id", "dwell_events", "queue_size", "throughput", "risk_score"]}, "code_snippet": "risk_score = min(100, dwell_events*2 + queue_size*3 - throughput)", "notes": "Computes a simple 0-100 congestion score using weighted factors"},
        {"stage_id": "load_scores", "stage_number": 5, "stage_name": "Load to PostgreSQL", "stage_type": "data_loading", "description": "Persist ports and disruption scores with indexes", "destination": {"table_name": "port_disruption_scores", "write_mode": "append", "create_indexes": true, "index_columns": ["port_id", "score_time"]}, "code_snippet": "from sqlalchemy import create_engine, text\nimport os\nengine = create_engine(os.environ['AIVEN_PG_URI'])\n# Create table if missing\nwith engine.connect() as conn:\n    conn.execute(text(\"CREATE TABLE IF NOT EXISTS port_disruption_scores (\\n        port_id TEXT,\\n        dwell_events INT,\\n        queue_size INT,\\n        throughput INT,\\n        risk_score INT,\\n        score_time TIMESTAMP DEFAULT NOW()\\n    )\"))\n    conn.commit()\n# Write rows\nscores_df.to_sql('port_disruption_scores', engine, if_exists='append', index=False)\n# Indexes\nwith engine.connect() as conn:\n    for col in ['port_id','score_time']:\n        conn.execute(text(f'CREATE INDEX IF NOT EXISTS idx_port_{col} ON port_disruption_scores({col})'))\n    conn.commit()", "notes": "Loads disruption scores and adds helpful indexes for filtering by port and time"}
      ],
      "final_output": {"database_table": "port_disruption_scores", "record_count": null}
    }
    ,
    {
      "pipeline_id": "crypto_prices",
      "pipeline_name": "Crypto Prices Snapshot",
      "description": "Fetch simple prices for a few coins and save a small table.",
      "source_type": "api",
      "status": "active",
      "last_run": null,
      "detailed_explanation": {
        "overview": "This pipeline gets current prices for a few popular cryptocurrencies and saves them. Its a tiny example that shows how to call an API and store a simple table.",
        "what_you_learn": [
          "How to fetch data from a public API",
          "Picking just the fields you need",
          "Saving a small snapshot to a database"
        ],
        "stage_details": [
          {"stage_number": 1, "what_happens": "Call the CoinGecko API for prices of BTC, ETH, and SOL.", "why_it_matters": "Shows a quick, real API request.", "technical_note": "HTTP GET + JSON parse."},
          {"stage_number": 2, "what_happens": "Make a small table with coin, price, currency, and time.", "why_it_matters": "Keeps the data clean and simple.", "technical_note": "Only a few columns."},
          {"stage_number": 3, "what_happens": "Save the rows to the database.", "why_it_matters": "Lets the app display the latest snapshot.", "technical_note": "Basic insert with indexes."}
        ]
      },
      "stages": [
        {
          "stage_id": "fetch_prices",
          "stage_number": 1,
          "stage_name": "Fetch Prices",
          "stage_type": "data_ingestion",
          "description": "Call CoinGecko API to get prices for several coins",
          "source": {
            "type": "api",
            "base_url": "https://api.coingecko.com/api/v3/simple/price",
            "coins": ["bitcoin", "ethereum", "solana", "cardano", "polkadot", "dogecoin", "litecoin", "tron", "chainlink", "monero"],
            "currency": "usd"
          },
          "output": {
            "format": "rows",
            "columns": ["coin", "currency", "price", "fetched_at"]
          },
          "code_snippet": "import urllib.request, json, pandas as pd\nurl = 'https://api.coingecko.com/api/v3/simple/price?ids=bitcoin,ethereum,solana,cardano,polkadot,dogecoin,litecoin,tron,chainlink,monero&vs_currencies=usd'\nreq = urllib.request.Request(url, headers={'User-Agent': 'DataJourney/1.0'})\nwith urllib.request.urlopen(req, timeout=10) as resp:\n    data = json.loads(resp.read().decode('utf-8'))\nfetched_at = pd.Timestamp.utcnow()",
          "notes": "Small API call with a short timeout to keep responses fast"
        },
        {
          "stage_id": "shape_table",
          "stage_number": 2,
          "stage_name": "Shape Table",
          "stage_type": "data_transformation",
          "description": "Create a tiny table with just the fields we need",
          "transformations": [
            {"operation": "select_fields", "description": "Keep coin, price, currency, and time"},
            {"operation": "normalize_names", "description": "Use simple lowercase names"}
          ],
          "output": {"format": "table", "columns": ["coin", "currency", "price", "fetched_at"]},
          "code_snippet": "rows = []\nfor coin, info in data.items():\n    price = info.get('usd')\n    if price is not None:\n        rows.append((coin, 'usd', float(price), fetched_at))\ncrypto_df = pd.DataFrame(rows, columns=['coin','currency','price','fetched_at'])"
        },
        {
          "stage_id": "load_to_db",
          "stage_number": 3,
          "stage_name": "Load to PostgreSQL",
          "stage_type": "data_loading",
          "description": "Insert the snapshot rows into crypto_prices table",
          "destination": {
            "table_name": "crypto_prices",
            "write_mode": "replace",
            "create_indexes": true,
            "index_columns": ["coin", "fetched_at"]
          },
          "code_snippet": "from sqlalchemy import create_engine, text\nimport os\nengine = create_engine(os.environ['AIVEN_PG_URI'])\nwith engine.connect() as conn:\n    conn.execute(text('TRUNCATE TABLE IF EXISTS crypto_prices'))\n    conn.commit()\ncrypto_df.to_sql('crypto_prices', engine, if_exists='append', index=False)\nwith engine.connect() as conn:\n    for col in ['coin','fetched_at']:\n        conn.execute(text(f'CREATE INDEX IF NOT EXISTS idx_crypto_{col} ON crypto_prices({col})'))\n    conn.commit()"
        }
      ],
      "final_output": {
        "database_table": "crypto_prices",
        "record_count": null
      }
    }
  ],
  "visualization_settings": {
    "dag_layout": "horizontal",
    "node_styling": {
      "data_ingestion": {
        "color": "#10B981",
        "icon": "download"
      },
      "data_cleaning": {
        "color": "#3B82F6",
        "icon": "clean"
      },
      "data_transformation": {
        "color": "#F59E0B",
        "icon": "transform"
      },
      "data_branching": {
        "color": "#EC4899",
        "icon": "branch"
      },
      "data_loading": {
        "color": "#8B5CF6",
        "icon": "database"
      },
      "parallel_processing": {
        "color": "#06B6D4",
        "icon": "parallel"
      }
    },
    "edge_styling": {
      "stroke": "#666",
      "stroke_width": 2,
      "arrow_size": 10
    },
    "interaction": {
      "enable_click": true,
      "enable_hover": true,
      "show_tooltips": true
    }
  }
}